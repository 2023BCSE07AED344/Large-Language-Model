from nltk.tokenize import word_tokenize
text = "Large LAnguage Models are transforming Natural Language Processing"
tokens = word_tokenize(text)
print(tokens)
